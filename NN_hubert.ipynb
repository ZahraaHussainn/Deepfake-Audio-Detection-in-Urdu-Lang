{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1ec267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import noisereduce as nr\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43f518fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\programdata\\anaconda3\\lib\\site-packages (2.4.1+cu118)\n",
      "Requirement already satisfied: torchaudio in c:\\programdata\\anaconda3\\lib\\site-packages (2.4.1+cu118)\n",
      "Requirement already satisfied: transformers in c:\\programdata\\anaconda3\\lib\\site-packages (4.45.2)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (4.66.5)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (1.8)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.0.12)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm) (0.4.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (1.1.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from networkx->torch) (5.0.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchaudio transformers tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7de2cdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2Processor, HubertForSequenceClassification\n",
    "# Load the pretrained HuBERT model and processor\n",
    "from transformers import AutoFeatureExtractor, HubertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18c89825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\programdata\\anaconda3\\lib\\site-packages (2.4.1+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\programdata\\anaconda3\\lib\\site-packages (0.19.1)\n",
      "Requirement already satisfied: torchaudio in c:\\programdata\\anaconda3\\lib\\site-packages (2.4.1+cu118)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2.5)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.0.12)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: sympy in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (1.8)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (8.2.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (1.22.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (1.1.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from networkx->torch) (5.0.6)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef18c43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14734ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Reshape\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import Wav2Vec2Processor, HubertModel, ViTConfig, ViTForImageClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "967ff360",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category: Bonafide...\n",
      "  Processing speaker folder: Speaker_01...\n",
      "    Processing part folder: Part 1...\n",
      "    Processing part folder: Part 2...\n",
      "  Processing speaker folder: Speaker_02...\n",
      "    Processing part folder: Part 1...\n",
      "    Processing part folder: Part 2...\n",
      "  Processing speaker folder: Speaker_03...\n",
      "    Processing part folder: Part 1...\n",
      "    Processing part folder: Part 2...\n",
      "  Processing speaker folder: Speaker_04...\n",
      "    Processing part folder: Part 1...\n",
      "    Processing part folder: Part 2...\n",
      "  Processing speaker folder: Speaker_05...\n",
      "    Processing part folder: Part 1...\n",
      "    Processing part folder: Part 2...\n",
      "  Processing speaker folder: Speaker_06...\n",
      "    Processing part folder: Part 1...\n",
      "    Processing part folder: Part 2...\n",
      "  Processing speaker folder: Speaker_07...\n",
      "    Processing part folder: Part 1...\n",
      "    Processing part folder: Part 2...\n",
      "  Processing speaker folder: Speaker_08...\n",
      "    Processing part folder: Part 1...\n",
      "    Processing part folder: Part 2...\n",
      "  Processing speaker folder: Speaker_09...\n",
      "    Processing part folder: Part 1...\n",
      "    Processing part folder: Part 2...\n",
      "  Processing speaker folder: Speaker_10...\n",
      "    Processing part folder: Part 1...\n",
      "    Processing part folder: Part 2...\n",
      "  Processing speaker folder: Speaker_11...\n",
      "    Processing part folder: Part 1...\n",
      "    Processing part folder: Part 2...\n",
      "  Processing speaker folder: Speaker_12...\n",
      "    Processing part folder: Part 1...\n",
      "    Processing part folder: Part 2...\n",
      "  Processing speaker folder: Speaker_13...\n",
      "    Processing part folder: Part 1...\n",
      "    Processing part folder: Part 2...\n",
      "  Processing speaker folder: Speaker_14...\n",
      "    Processing part folder: Part 1...\n",
      "    Processing part folder: Part 2...\n",
      "  Processing speaker folder: Speaker_15...\n",
      "    Processing part folder: Part 1...\n",
      "    Processing part folder: Part 2...\n",
      "  Processing speaker folder: Speaker_16...\n",
      "    Processing part folder: Part 1...\n",
      "    Processing part folder: Part 2...\n",
      "  Processing speaker folder: Speaker_17...\n",
      "    Processing part folder: Part 1...\n",
      "    Processing part folder: Part 2...\n",
      "Processing category: Spoofed_TTS...\n",
      "  Processing speaker folder: Speaker_01...\n",
      "  Processing speaker folder: Speaker_02...\n",
      "  Processing speaker folder: Speaker_03...\n",
      "  Processing speaker folder: Speaker_04...\n",
      "  Processing speaker folder: Speaker_05...\n",
      "  Processing speaker folder: Speaker_06...\n",
      "  Processing speaker folder: Speaker_07...\n",
      "  Processing speaker folder: Speaker_08...\n",
      "  Processing speaker folder: Speaker_09...\n",
      "  Processing speaker folder: Speaker_10...\n",
      "  Processing speaker folder: Speaker_11...\n",
      "  Processing speaker folder: Speaker_12...\n",
      "  Processing speaker folder: Speaker_13...\n",
      "  Processing speaker folder: Speaker_14...\n",
      "  Processing speaker folder: Speaker_15...\n",
      "  Processing speaker folder: Speaker_16...\n",
      "  Processing speaker folder: Speaker_17...\n",
      "Processing category: Spoofed_Tacotron...\n",
      "  Processing speaker folder: Speaker_01...\n",
      "  Processing speaker folder: Speaker_02...\n",
      "  Processing speaker folder: Speaker_03...\n",
      "  Processing speaker folder: Speaker_04...\n",
      "  Processing speaker folder: Speaker_05...\n",
      "  Processing speaker folder: Speaker_06...\n",
      "  Processing speaker folder: Speaker_07...\n",
      "  Processing speaker folder: Speaker_08...\n",
      "  Processing speaker folder: Speaker_09...\n",
      "  Processing speaker folder: Speaker_10...\n",
      "  Processing speaker folder: Speaker_11...\n",
      "  Processing speaker folder: Speaker_12...\n",
      "  Processing speaker folder: Speaker_13...\n",
      "  Processing speaker folder: Speaker_14...\n",
      "  Processing speaker folder: Speaker_15...\n",
      "  Processing speaker folder: Speaker_16...\n",
      "  Processing speaker folder: Speaker_17...\n",
      "Total audio files: 6794\n",
      "First 5 file paths: ['D:\\\\fast\\\\fyp\\\\DATASET\\\\Bonafide\\\\Speaker_01\\\\Part 1\\\\10.wav'\n",
      " 'D:\\\\fast\\\\fyp\\\\DATASET\\\\Bonafide\\\\Speaker_01\\\\Part 1\\\\100.wav'\n",
      " 'D:\\\\fast\\\\fyp\\\\DATASET\\\\Bonafide\\\\Speaker_01\\\\Part 1\\\\101.wav'\n",
      " 'D:\\\\fast\\\\fyp\\\\DATASET\\\\Bonafide\\\\Speaker_01\\\\Part 1\\\\102.wav'\n",
      " 'D:\\\\fast\\\\fyp\\\\DATASET\\\\Bonafide\\\\Speaker_01\\\\Part 1\\\\103.wav']\n",
      "First 5 labels: [1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Function to recursively collect .wav files and their labels\n",
    "def load_dataset(base_path):\n",
    "    file_paths = []\n",
    "    labels = []\n",
    "    categories = {'Bonafide': 1, 'Spoofed_TTS': 0, 'Spoofed_Tacotron': 0}  # Map categories to labels\n",
    "\n",
    "    # Debug: Check if the base path exists\n",
    "    if not os.path.exists(base_path):\n",
    "        print(f\"Error: The base path {base_path} does not exist.\")\n",
    "        return file_paths, labels\n",
    "\n",
    "    for category, label in categories.items():\n",
    "        category_path = os.path.join(base_path, category)\n",
    "\n",
    "        # Debug: Check if the category folder exists\n",
    "        if not os.path.exists(category_path):\n",
    "            print(f\"Error: The category folder {category_path} does not exist.\")\n",
    "            continue  # Skip this category if the folder is missing\n",
    "\n",
    "        print(f\"Processing category: {category}...\")\n",
    "\n",
    "        for speaker_folder in os.listdir(category_path):\n",
    "            speaker_path = os.path.join(category_path, speaker_folder)\n",
    "\n",
    "            # Debug: Check if the folder is a directory\n",
    "            if os.path.isdir(speaker_path):\n",
    "                print(f\"  Processing speaker folder: {speaker_folder}...\")\n",
    "                # Special case for Bonafide: Handle nested Part 1 and Part 2 folders\n",
    "                if category == 'Bonafide':\n",
    "                    for part_folder in os.listdir(speaker_path):\n",
    "                        part_path = os.path.join(speaker_path, part_folder)\n",
    "                        if os.path.isdir(part_path):\n",
    "                            print(f\"    Processing part folder: {part_folder}...\")\n",
    "                            for file_name in os.listdir(part_path):\n",
    "                                if file_name.endswith('.wav'):\n",
    "                                    file_path = os.path.join(part_path, file_name)\n",
    "                                    file_paths.append(file_path)\n",
    "                                    labels.append(label)\n",
    "                else:\n",
    "                    # Handle non-nested folders for Spoofed_TTS and Spoofed_Tacotron\n",
    "                    for file_name in os.listdir(speaker_path):\n",
    "                        if file_name.endswith('.wav'):\n",
    "                            file_path = os.path.join(speaker_path, file_name)\n",
    "                            file_paths.append(file_path)\n",
    "                            labels.append(label)\n",
    "            else:\n",
    "                print(f\"  Skipping non-directory: {speaker_folder}\")\n",
    "\n",
    "    return np.array(file_paths), np.array(labels)\n",
    "\n",
    "# Set the base path where your dataset is stored\n",
    "base_path = r'D:\\fast\\fyp\\DATASET'  # Update path to your dataset location\n",
    "\n",
    "# Load dataset\n",
    "file_paths, labels = load_dataset(base_path)\n",
    "\n",
    "# Display the loaded data for debugging\n",
    "print(f\"Total audio files: {len(file_paths)}\")\n",
    "print(f\"First 5 file paths: {file_paths[:5]}\")\n",
    "print(f\"First 5 labels: {labels[:5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "808ddcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_audio_files(folder_path):\n",
    "    return [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.wav')]\n",
    "\n",
    "# Organize dataset into categories\n",
    "audio_files = {'Bonafide': [], 'Spoofed_TTS': [], 'Spoofed_Tacotron': []}\n",
    "for category in ['Bonafide', 'Spoofed_TTS', 'Spoofed_Tacotron']:\n",
    "    category_path = os.path.join(base_path, category)\n",
    "\n",
    "    for speaker_folder in os.listdir(category_path):\n",
    "        speaker_path = os.path.join(category_path, speaker_folder)\n",
    "\n",
    "        # Handle Bonafide folder with Part 1 and Part 2 subfolders\n",
    "        if category == 'Bonafide':\n",
    "            for part in ['Part 1', 'Part 2']:\n",
    "                part_path = os.path.join(speaker_path, part)\n",
    "                audio_files[category].extend(list_audio_files(part_path))\n",
    "        else:\n",
    "            audio_files[category].extend(list_audio_files(speaker_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fbed32f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cf59bf1959f4bb6ba1e39ed663b533f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/212 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\FAST\\.cache\\huggingface\\hub\\models--facebook--hubert-large-ls960-ft. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c3bf108dde447c87d0ac8089268a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/138 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93c4f52cb5c54ef4b4a4513f69e92275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7387cc9e05984d4da333eb5b476e86c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "562c41a8934a4a89aa3232aeb8dee1b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56a4b13182c94b2eac4bd7192826a56b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting HuBERT embeddings:   0%|                                                           | 0/5435 [00:00<?, ?it/s]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\models\\hubert\\modeling_hubert.py:762: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Extracting HuBERT embeddings: 100%|████████████████████████████████████████████████| 5435/5435 [05:55<00:00, 15.27it/s]\n",
      "Extracting HuBERT embeddings: 100%|████████████████████████████████████████████████| 1359/1359 [01:28<00:00, 15.39it/s]\n"
     ]
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85cb29cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09ee8dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding length: 1024\n",
      "Target size: 1024\n",
      "Height and Width: 32, 32\n",
      "x_train_square shape: (5435, 32, 32)\n",
      "x_test_square shape: (1359, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Correct embedding length calculation\n",
    "embedding_length = x_train.shape[1]  # Length of each embedding vector\n",
    "print(f\"Embedding length: {embedding_length}\")\n",
    "\n",
    "# Ensure target size is the next perfect square if needed\n",
    "target_size = math.ceil(math.sqrt(embedding_length)) ** 2\n",
    "height = width = int(math.sqrt(target_size))\n",
    "\n",
    "# Debugging target size\n",
    "print(f\"Target size: {target_size}\")\n",
    "print(f\"Height and Width: {height}, {width}\")\n",
    "\n",
    "# Function to pad embeddings to make them square\n",
    "def pad_to_square(embeddings, target_size):\n",
    "    padded_embeddings = []\n",
    "    for embedding in embeddings:\n",
    "        # Padding to match the target size\n",
    "        padding_length = target_size - embedding.shape[0]\n",
    "        padded_embedding = np.pad(embedding, (0, padding_length), mode=\"constant\")\n",
    "        padded_embeddings.append(padded_embedding.reshape(height, width))  # Reshape to 2D\n",
    "    return np.array(padded_embeddings)\n",
    "\n",
    "# Pad and reshape embeddings for training and testing\n",
    "x_train_square = pad_to_square(x_train, target_size)\n",
    "x_test_square = pad_to_square(x_test, target_size)\n",
    "\n",
    "# Verify shapes after padding and reshaping\n",
    "print(f\"x_train_square shape: {x_train_square.shape}\")  # Expected: (n_samples, height, width)\n",
    "print(f\"x_test_square shape: {x_test_square.shape}\")    # Expected: (n_samples, height, width)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9754f72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Custom Dataset class\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = torch.tensor(embeddings, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.labels[idx]\n",
    "\n",
    "# Prepare Dataset\n",
    "train_dataset = AudioDataset(x_train_square, y_train)\n",
    "test_dataset = AudioDataset(x_test_square, y_test)\n",
    "\n",
    "# DataLoader\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c7b49cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "# Define Vision Transformer Model\n",
    "class ViTForAudio(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, patch_size=4, num_layers=6, num_heads=8, hidden_dim=256):\n",
    "        super(ViTForAudio, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.patch_size = patch_size\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Transformer encoder\n",
    "        self.embedding = nn.Conv2d(1, hidden_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, (input_dim // patch_size) ** 2, hidden_dim))\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads), num_layers=num_layers\n",
    "        )\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.unsqueeze(1)  # Add channel dimension\n",
    "        x = self.embedding(x)  # (B, hidden_dim, H/patch, W/patch)\n",
    "        x = x.flatten(2).transpose(1, 2)  # (B, num_patches, hidden_dim)\n",
    "        x = x + self.positional_encoding  # Add positional encoding\n",
    "        x = self.transformer(x)  # Transformer encoding\n",
    "        x = x.mean(dim=1)  # Global average pooling\n",
    "        x = self.classifier(x)  # Classification\n",
    "        return x\n",
    "\n",
    "# Initialize the Model\n",
    "input_dim = x_train_square.shape[1]  # Height/Width of reshaped embeddings\n",
    "num_classes = 2\n",
    "model = ViTForAudio(input_dim, num_classes).to(\"cuda\")\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "000ae300",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|████████████████████████████████████████████████████████████████████| 170/170 [00:04<00:00, 41.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.7498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|████████████████████████████████████████████████████████████████████| 170/170 [00:03<00:00, 45.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss = 0.7013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|████████████████████████████████████████████████████████████████████| 170/170 [00:03<00:00, 45.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss = 0.6992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|████████████████████████████████████████████████████████████████████| 170/170 [00:03<00:00, 45.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss = 0.6969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|████████████████████████████████████████████████████████████████████| 170/170 [00:03<00:00, 44.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss = 0.6981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|████████████████████████████████████████████████████████████████████| 170/170 [00:03<00:00, 44.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss = 0.6951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|████████████████████████████████████████████████████████████████████| 170/170 [00:03<00:00, 44.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss = 0.6943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|████████████████████████████████████████████████████████████████████| 170/170 [00:03<00:00, 44.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss = 0.6940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|████████████████████████████████████████████████████████████████████| 170/170 [00:03<00:00, 44.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss = 0.6946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|███████████████████████████████████████████████████████████████████| 170/170 [00:03<00:00, 44.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss = 0.6938\n",
      "Accuracy: 0.5011\n",
      "F1 Score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for embeddings, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        embeddings, labels = embeddings.to(\"cuda\"), labels.to(\"cuda\")\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(embeddings)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}: Train Loss = {train_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for embeddings, labels in test_loader:\n",
    "        embeddings, labels = embeddings.to(\"cuda\"), labels.to(\"cuda\")\n",
    "        outputs = model(embeddings)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predictions.cpu().numpy())\n",
    "\n",
    "# Compute Metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cd8d45",
   "metadata": {},
   "source": [
    "FINE TUNED VIT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81255f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning Vision Transformer\n",
    "\n",
    "# Adjusting ViT model with dropout for regularization\n",
    "class ViTForAudioFineTuned(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, patch_size=4, num_layers=6, num_heads=8, hidden_dim=256, dropout=0.1):\n",
    "        super(ViTForAudioFineTuned, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.patch_size = patch_size\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.embedding = nn.Conv2d(1, hidden_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, (input_dim // patch_size) ** 2, hidden_dim))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads, dropout=dropout)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.unsqueeze(1)  # Add channel dimension\n",
    "        x = self.embedding(x)  # (B, hidden_dim, H/patch, W/patch)\n",
    "        x = x.flatten(2).transpose(1, 2)  # (B, num_patches, hidden_dim)\n",
    "        x = x + self.positional_encoding  # Add positional encoding\n",
    "        x = self.transformer(x)  # Transformer encoding\n",
    "        x = x.mean(dim=1)  # Global average pooling\n",
    "        x = self.classifier(x)  # Classification\n",
    "        return x\n",
    "\n",
    "# Re-initialize the model\n",
    "fine_tuned_model = ViTForAudioFineTuned(input_dim, num_classes).to(\"cuda\")\n",
    "\n",
    "# Optimizer with weight decay and learning rate scheduler\n",
    "optimizer = optim.AdamW(fine_tuned_model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "# Loss function with class weights (if imbalance exists)\n",
    "class_weights = torch.tensor([1.0, 1.0]).to(\"cuda\")  # Adjust if imbalance is present\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Training loop with fine-tuning\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    fine_tuned_model.train()\n",
    "    train_loss = 0\n",
    "    for embeddings, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        embeddings, labels = embeddings.to(\"cuda\"), labels.to(\"cuda\")\n",
    "        optimizer.zero_grad()\n",
    "        outputs = fine_tuned_model(embeddings)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch + 1}: Train Loss = {train_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Evaluation with precision and F1 score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "fine_tuned_model.eval()\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for embeddings, labels in test_loader:\n",
    "        embeddings, labels = embeddings.to(\"cuda\"), labels.to(\"cuda\")\n",
    "        outputs = fine_tuned_model(embeddings)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predictions.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f2cd44",
   "metadata": {},
   "source": [
    "# new fine tuned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5872ac01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust class weights based on the dataset\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "classes = np.unique(y_train)\n",
    "class_weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
    "class_weights = torch.tensor(class_weights).float().to(\"cuda\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1303b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping utility\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        torch.save(model.state_dict(), \"checkpoint.pt\")\n",
    "\n",
    "# Initialize EarlyStopping\n",
    "early_stopping = EarlyStopping(patience=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a4c6761",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|████████████████████████████████████████████████████████████████████| 170/170 [00:04<00:00, 41.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6930, Val Loss = 0.6932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|████████████████████████████████████████████████████████████████████| 170/170 [00:03<00:00, 43.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss = 0.6931, Val Loss = 0.6932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|████████████████████████████████████████████████████████████████████| 170/170 [00:03<00:00, 43.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss = 0.6935, Val Loss = 0.6932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|████████████████████████████████████████████████████████████████████| 170/170 [00:03<00:00, 44.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss = 0.6931, Val Loss = 0.6932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|████████████████████████████████████████████████████████████████████| 170/170 [00:03<00:00, 43.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss = 0.6936, Val Loss = 0.6932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|████████████████████████████████████████████████████████████████████| 170/170 [00:03<00:00, 43.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss = 0.6935, Val Loss = 0.6932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|████████████████████████████████████████████████████████████████████| 170/170 [00:03<00:00, 43.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss = 0.6939, Val Loss = 0.6932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|████████████████████████████████████████████████████████████████████| 170/170 [00:03<00:00, 43.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss = 0.6931, Val Loss = 0.6932\n",
      "Early stopping triggered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-512266e72c3d>:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  fine_tuned_model.load_state_dict(torch.load(\"checkpoint.pt\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 20  # Increased epochs\n",
    "for epoch in range(num_epochs):\n",
    "    fine_tuned_model.train()\n",
    "    train_loss = 0\n",
    "    for embeddings, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        embeddings, labels = embeddings.to(\"cuda\"), labels.to(\"cuda\")\n",
    "        optimizer.zero_grad()\n",
    "        outputs = fine_tuned_model(embeddings)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validation loop\n",
    "    fine_tuned_model.eval()\n",
    "    val_loss = 0\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for embeddings, labels in test_loader:\n",
    "            embeddings, labels = embeddings.to(\"cuda\"), labels.to(\"cuda\")\n",
    "            outputs = fine_tuned_model(embeddings)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predictions.cpu().numpy())\n",
    "\n",
    "    val_loss /= len(test_loader)\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch + 1}: Train Loss = {train_loss / len(train_loader):.4f}, Val Loss = {val_loss:.4f}\")\n",
    "\n",
    "    # Check Early Stopping\n",
    "    early_stopping(val_loss, fine_tuned_model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "# Load the best model checkpoint\n",
    "fine_tuned_model.load_state_dict(torch.load(\"checkpoint.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0f325bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[  0 681]\n",
      " [  0 678]]\n",
      "Accuracy: 0.4989\n",
      "F1 Score: 0.6657\n",
      "Precision: 0.4989\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       681\n",
      "           1       0.50      1.00      0.67       678\n",
      "\n",
      "    accuracy                           0.50      1359\n",
      "   macro avg       0.25      0.50      0.33      1359\n",
      "weighted avg       0.25      0.50      0.33      1359\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753377ed",
   "metadata": {},
   "source": [
    "# new approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca207bb6",
   "metadata": {},
   "source": [
    "MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48d645ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 0.5683\n",
      "Epoch 1/20, Accuracy: 0.8300, Precision: 0.8331, F1 Score: 0.8288\n",
      "Epoch 2/20, Train Loss: 0.3729\n",
      "Epoch 2/20, Accuracy: 0.8882, Precision: 0.9188, F1 Score: 0.8836\n",
      "Epoch 3/20, Train Loss: 0.2696\n",
      "Epoch 3/20, Accuracy: 0.8896, Precision: 0.8359, F1 Score: 0.8975\n",
      "Epoch 4/20, Train Loss: 0.2305\n",
      "Epoch 4/20, Accuracy: 0.9323, Precision: 0.9234, F1 Score: 0.9328\n",
      "Epoch 5/20, Train Loss: 0.1992\n",
      "Epoch 5/20, Accuracy: 0.9411, Precision: 0.9164, F1 Score: 0.9427\n",
      "Epoch 6/20, Train Loss: 0.1749\n",
      "Epoch 6/20, Accuracy: 0.9514, Precision: 0.9527, F1 Score: 0.9513\n",
      "Epoch 7/20, Train Loss: 0.1294\n",
      "Epoch 7/20, Accuracy: 0.9610, Precision: 0.9685, F1 Score: 0.9606\n",
      "Epoch 8/20, Train Loss: 0.1432\n",
      "Epoch 8/20, Accuracy: 0.9551, Precision: 0.9303, F1 Score: 0.9563\n",
      "Epoch 9/20, Train Loss: 0.1046\n",
      "Epoch 9/20, Accuracy: 0.9603, Precision: 0.9727, F1 Score: 0.9596\n",
      "Epoch 10/20, Train Loss: 0.1259\n",
      "Epoch 10/20, Accuracy: 0.9669, Precision: 0.9803, F1 Score: 0.9663\n",
      "Epoch 11/20, Train Loss: 0.1012\n",
      "Epoch 11/20, Accuracy: 0.9294, Precision: 0.9949, F1 Score: 0.9242\n",
      "Epoch 12/20, Train Loss: 0.1283\n",
      "Epoch 12/20, Accuracy: 0.9029, Precision: 0.9964, F1 Score: 0.8925\n",
      "Epoch 13/20, Train Loss: 0.1292\n",
      "Epoch 13/20, Accuracy: 0.9485, Precision: 0.9903, F1 Score: 0.9461\n",
      "Epoch 14/20, Train Loss: 0.0848\n",
      "Epoch 14/20, Accuracy: 0.9617, Precision: 0.9875, F1 Score: 0.9606\n",
      "Epoch 15/20, Train Loss: 0.0995\n",
      "Epoch 15/20, Accuracy: 0.9581, Precision: 0.9829, F1 Score: 0.9569\n",
      "Epoch 16/20, Train Loss: 0.0767\n",
      "Epoch 16/20, Accuracy: 0.9654, Precision: 0.9463, F1 Score: 0.9661\n",
      "Epoch 17/20, Train Loss: 0.0838\n",
      "Epoch 17/20, Accuracy: 0.9478, Precision: 0.9951, F1 Score: 0.9450\n",
      "Epoch 18/20, Train Loss: 0.0708\n",
      "Epoch 18/20, Accuracy: 0.9750, Precision: 0.9600, F1 Score: 0.9753\n",
      "Epoch 19/20, Train Loss: 0.0720\n",
      "Epoch 19/20, Accuracy: 0.9529, Precision: 0.9171, F1 Score: 0.9547\n",
      "Epoch 20/20, Train Loss: 0.0659\n",
      "Epoch 20/20, Accuracy: 0.8801, Precision: 0.9981, F1 Score: 0.8636\n",
      "Final Accuracy: 0.8801, Final Precision: 0.9981, Final F1 Score: 0.8636\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Step 1: Define a fully connected neural network (MLP)\n",
    "class MLPForEmbeddings(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=512, output_dim=2):\n",
    "        super(MLPForEmbeddings, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Step 2: Convert embeddings to torch tensors\n",
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32).to('cuda')\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long).to('cuda')\n",
    "x_test_tensor = torch.tensor(x_test, dtype=torch.float32).to('cuda')\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long).to('cuda')\n",
    "\n",
    "# Step 3: Create data loaders\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Step 4: Initialize the model, loss function, and optimizer\n",
    "input_dim = x_train.shape[1]  # Number of features (dimensions of embeddings)\n",
    "model = MLPForEmbeddings(input_dim).to('cuda')\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Step 5: Training loop\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print training loss\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Step 6: Evaluate on the test set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        for batch in test_loader:\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Calculate accuracy, precision, and F1 score\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        precision = precision_score(all_labels, all_preds)\n",
    "        f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Step 7: Final evaluation on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for batch in test_loader:\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Final metrics\n",
    "    final_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    final_precision = precision_score(all_labels, all_preds)\n",
    "    final_f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "    print(f\"Final Accuracy: {final_accuracy:.4f}, Final Precision: {final_precision:.4f}, Final F1 Score: {final_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da35058",
   "metadata": {},
   "source": [
    "# final error check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250a668f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
